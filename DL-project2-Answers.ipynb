{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    " Inkawhich, Nathan. “DCGAN Tutorial¶.” DCGAN Tutorial - PyTorch Tutorials 1.7.1 Documentation, Pytorch, 2017, pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html?fbclid=IwAR3UQJhOBXg1YfPO0hhoHShO2NN4--mnaGb0qTGefjcqg5rp0PMpsbK6HFQ. \n",
    " \n",
    " powerpoints on nyu classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network (GAN)\n",
    "Since GANs were introduced in 2014 by Google Researcher Ian Goodfellow, the tech has been widely adopted in image generation and transfer. After some early wiry failures, GANs have made huge breakthroughs and can now produce highly convincing fake images of animals, landscapes, human faces, etc. Researchers know what GANs can do, however a lack of transparency in their inner workings means GAN improvement is still achieved mainly through trial-and-error. This allows only limited control over the synthesized images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Packages\n",
    "Let's first import the necessary packages,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets as dat_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## GPU Device Configuration\n",
    "Then, we set up and configure our computational devices: \n",
    "Whether we use GPU or perform the calculation on CPU.\n",
    "we use the torch.devices() and torch.cude.is_available() functions to configure our computational devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "comp_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(comp_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Configuration\n",
    "### hyper parameters\n",
    "We then set up and hyper parameters that need for the our model.\n",
    "we need to define several hyper parameters for our model:\n",
    "1. latent size\n",
    "2. hidden size\n",
    "3. input image size\n",
    "4. numbper of epoches\n",
    "5. batch size\n",
    "6. out put directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rate = 0.0001\n",
    "hidden_size=256\n",
    "latent_size=100\n",
    "image_size=64*64*3\n",
    "batch_size = 32\n",
    "n_epoch = 20 #can be changed, used for efficiency\n",
    "filename = \"./out_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory if not exists\n",
    "using os.path.exists() to check whether it is exist\n",
    "using os.makedires to create a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(filename):\n",
    "    os.makedirs(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "##  Image processing\n",
    "Then, we define a image preprocessing object that our dataloader can directly use this object to preprocess our data\n",
    "We use the pytorch API to preform the data processing.\n",
    "1. Use transforms.Compose()\n",
    "2. Use transforms.CenterCrop(160) \n",
    "3. Use transforms.Scale(64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "celebs = dat_s.ImageFolder(root=\"/home/mmvc/img\", transform=transforms.Compose([\n",
    "                            transforms.CenterCrop(160),   \n",
    "                            transforms.Resize(64),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "##  Data Loading\n",
    "Next, we are going to load our data. \n",
    "### First, we need to prepare our data:\n",
    "#### we use the following command to download our data:\n",
    "1. apt-get install p7zip-full # ubuntu\n",
    "\n",
    "2. brew install p7zip # Mac\n",
    "\n",
    "3. python download.py\n",
    "-----\n",
    "### We first import necessary librarys for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.datasets as dset\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We first define several helper functions that can help use to load each item in the dataset .\n",
    "1. We first create a list that contains all image files.    '.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',.\n",
    "\n",
    "2. We then define a function called is_image_file() which takes the file name as the input:\n",
    "    <br />a. We 1 if it is a valid image file.\n",
    "    <br />b. Otherwise we return 0.\n",
    "    \n",
    "3. We next define a make_dataset() function which takes a file path as the input:\n",
    "    <br />a. We go over the path\n",
    "    <br />b. If it is a valid img file, store it to a list\n",
    "    <br />c. Return the list\n",
    "    \n",
    "4. Finally, we create a function that is called default_loader()\n",
    "    <br />a. that will open the image and conver it to the RGB using Image.open() and convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_img = torch.utils.data.DataLoader(celebs, batch_size=1,shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We then can use those helper functions to create our dataloader that load each item.  This function is called ImageFloder(data.Dataset) \n",
    "1. This function is initlize with root, transform, traget transform and loader\n",
    "    <br /> a. get all imgs using the correct func we define above\n",
    "    <br /> b. if no valide imgs: raise an proper error\n",
    "    <br /> c. print the length of valid data\n",
    "\n",
    "\n",
    "\n",
    "2. We need to define a __getitem__() function that take index as input\n",
    "    <br /> a. for the \"index\" element in the img list\n",
    "         i. if the transform is not none\n",
    "             we transform the img using the transofrm\n",
    "         ii. if the traget transform is not none:\n",
    "             same.\n",
    "    <br /> b. return the img and target.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. We deine a __len__() function that retrun the length of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64, 64])\n",
      "Dataset Size:  202599\n"
     ]
    }
   ],
   "source": [
    "img, target = dataloader_img.dataset.__getitem__(202598)\n",
    "print(img.size())\n",
    "\n",
    "print(\"Dataset Size: \", len(dataloader_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We then define our data loader get_loader()\n",
    "#### This func has 6 arguments\n",
    "1. root\n",
    "2. split\n",
    "3. batch size\n",
    "4. scale size\n",
    "5. number of workers\n",
    "6. shuffle\n",
    "\n",
    "\n",
    "\n",
    "<br />We first store the return value of os.path.basename of root to a variable\n",
    "<br />Then we store the path to the image root\n",
    "        <br /> 1. if the dataset_name is in CelebA:\n",
    "        <br /> 2. then, we using the ImageFolder object we define above with transform.CenterCrop(160) to store the data\n",
    "        <br /> 3. otherwise, we do not add the transfrom.CenterCrop when storing the data\n",
    "<br />Finally, we create a data_loade using torch.utils.data.DataLoader() with proper parameters and set the proper shape of this data_loader using data_loade.shape=?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the function above to load the data to the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = data.DataLoader(dataset = celebs, batch_size = batch_size, shuffle=True, num_workers=2, drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "##  Network\n",
    "Next, we are going to design our GAN\n",
    "We use the pytorch function nn.Sequential() to stack several layers as well as activation functions\n",
    "### First, we need to create our discriminator\n",
    "1. We need one input layer, one hidden layers and one out put layer. All of them are defined using nn.Linear() with proper input dim and out dim\n",
    "\n",
    "\n",
    "2. We adopt nn.LeakyReLU(0.2) as activation layer for the input and hidden layer. \n",
    "\n",
    "\n",
    "3. We use nn.Sigmoid() activation function for the output layer\n",
    "\n",
    "### Next, we are going to define our generator \n",
    "1. We need one input layer, one hidden layers and one out put layer. All of them are defined using nn.Linear() with proper input dim and out dim\n",
    "\n",
    "\n",
    "2. We adopt nn.ReLU() as activation layer for the input and hidden layer. \n",
    "\n",
    "\n",
    "3. We use nn.Tanh() activation function for the output layer\n",
    "\n",
    "\n",
    "#### Please First construct the generator module as follow:\n",
    "1.\tThe input of the first linear layer is the latent vector size, output of the first layer is 256\n",
    "2.\tThen followed by a ReLU layer.\n",
    "3.\tThe input of the second layer is the 256 and output channel is 512\n",
    "4.\tFollowed by the ReLU layer\n",
    "5.\tThe input of the third layer is the 512 and output channel is 1024\n",
    "6.\tFollowed by the ReLU layer\n",
    "7.\tThe input of the fourth layer is the 1024 and output channel is 1024\n",
    "8.\tFollowed by the ReLU layer\n",
    "9.\tThe input of the final layer is the 1024 and output channel is the image size.\n",
    "10.\tThe Tanh is activation function.\n",
    "\n",
    "#### Please First construct the discriminator module as follow:\n",
    "1.\tThe input of the first linear layer is the image size, output of the first layer is 256\n",
    "2.\tThen followed by a LeakyReLU layer.\n",
    "3.\tThe input of the second layer is the 256 and output channel is 512\n",
    "4.\tFollowed by the leakyReLU layer\n",
    "5.\tThe input of the third layer is the 512 and output channel is 512\n",
    "6.\tFollowed by the leakyReLU layer\n",
    "7.\tThe input of the final layer is the 512 and output channel is 1\n",
    "8.\tThe sigmoid is activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(latent_size, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, image_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(image_size, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we send the network to the target device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkg = Generator().to(comp_device)\n",
    "networkd = Discriminator().to(comp_device)\n",
    "\n",
    "if comp_device == 'cuda':\n",
    "    networkg = torch.nn.DataParallel(networkg)\n",
    "    networkd = torch.nn.DataParallel(networkd)\n",
    "    cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, We set the Binary cross entropy loss and optimizer with proper netwrok parameters and learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerd = optim.Adam(networkd.parameters(), lr=l_rate)\n",
    "optimizerg = optim.SGD(networkg.parameters(), lr=l_rate)\n",
    "\n",
    "optimizerd.zero_grad()\n",
    "optimizerg.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "##  Training\n",
    "Then, we are going to train our Network\n",
    "### We first starting with two helper function\n",
    "1. We frist implement the denorm function using clamp() api from pytorch please refer https://pytorch.org/docs/stable/torch.html?highlight=clamp#torch.clamp\n",
    "2. We define a function that reset all the gradient of the optimziers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training\n",
    "1. we first store the total steps which is equal to the length of data_loader\n",
    "2. for each epoch\n",
    "    <br/> a. for each element index and element in the data loader\n",
    "        i. we reshape the input data to (batch_size，-1) and send to the proper device\n",
    "        ii. then we create the real and fake labels which are later used as input for the BCE loss using torch.ones(batch_size, 1).to(device) and torch.zeros(batch_size, 1).to(device)\n",
    "        iii. then we train the descriminator\n",
    "            A. feedforward and store the predictions of discriminator\n",
    "            B. compute BCE_Loss using real images and store the loss\n",
    "            C. random init a latent code z\n",
    "            D. feedforward and store the predictions of generator\n",
    "            E. feed the predictions to the descriminator and sotre the prediction\n",
    "            F. compute BCELoss using fake images and store the loss\n",
    "            G. perofrm the backprop using losses after that reset the gradient of optimzier.\n",
    "        iv. then we train the generator\n",
    "            A. using torch.randn(batch_size, latent_size).to(device) to init a z\n",
    "            B. feedforward and store the predictions of generator\n",
    "            C. feed the predictions to the descriminator and sotre the prediction\n",
    "            D. compute BCE_Loss using real images and store the loss\n",
    "            F. perofrm the backprop using losses after that reset the gradient of optimzier.\n",
    "        v. some centain period, we prient the log with proper info\n",
    "    <br/> b. we store all real image with shape (images.size(0), 1, 28, 28) inoder for the further comparision only once\n",
    "    <br/> c. we save the fake image with image shape (images.size(0), 1, 28, 28) and denorm() function \n",
    "3. Save the model checkpoints using torch.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Index 0, GenLoss 0.6888335347175598, DiscLoss 1.3745349645614624\n",
      "\n",
      "Epoch 1, Index 500, GenLoss 6.181506156921387, DiscLoss 0.002157350303605199\n",
      "\n",
      "Epoch 1, Index 1000, GenLoss 7.870314598083496, DiscLoss 0.0004001879133284092\n",
      "\n",
      "Epoch 1, Index 1500, GenLoss 6.234555244445801, DiscLoss 0.002513469662517309\n",
      "\n",
      "Epoch 1, Index 2000, GenLoss 4.200990200042725, DiscLoss 0.0089495237916708\n",
      "\n",
      "Epoch 1, Index 2500, GenLoss 4.362092018127441, DiscLoss 0.05449609458446503\n",
      "\n",
      "Epoch 1, Index 3000, GenLoss 6.2295637130737305, DiscLoss 0.0019622338004410267\n",
      "\n",
      "Epoch 1, Index 3500, GenLoss 7.086174964904785, DiscLoss 0.0012091835960745811\n",
      "\n",
      "Epoch 1, Index 4000, GenLoss 6.826720237731934, DiscLoss 0.001345864380709827\n",
      "\n",
      "Epoch 1, Index 4500, GenLoss 12.198116302490234, DiscLoss 5.7220640883315355e-06\n",
      "\n",
      "Epoch 1, Index 5000, GenLoss 7.7083516120910645, DiscLoss 0.000528842443600297\n",
      "\n",
      "Epoch 1, Index 5500, GenLoss 8.206317901611328, DiscLoss 0.001208184752613306\n",
      "\n",
      "Epoch 1, Index 6000, GenLoss 8.654129028320312, DiscLoss 0.0014721204061061144\n",
      "\n",
      "Epoch 2, Index 0, GenLoss 9.17844009399414, DiscLoss 0.0007860098849050701\n",
      "\n",
      "Epoch 2, Index 500, GenLoss 5.594577312469482, DiscLoss 0.00749664381146431\n",
      "\n",
      "Epoch 2, Index 1000, GenLoss 8.783548355102539, DiscLoss 0.0007353659020736814\n",
      "\n",
      "Epoch 2, Index 1500, GenLoss 11.222551345825195, DiscLoss 1.785733547876589e-05\n",
      "\n",
      "Epoch 2, Index 2000, GenLoss 6.9305419921875, DiscLoss 0.07239997386932373\n",
      "\n",
      "Epoch 2, Index 2500, GenLoss 6.450374126434326, DiscLoss 0.003837929805740714\n",
      "\n",
      "Epoch 2, Index 3000, GenLoss 4.587868690490723, DiscLoss 0.013892171904444695\n",
      "\n",
      "Epoch 2, Index 3500, GenLoss 7.9871039390563965, DiscLoss 0.0006178017938509583\n",
      "\n",
      "Epoch 2, Index 4000, GenLoss 4.944547653198242, DiscLoss 0.0058431378565728664\n",
      "\n",
      "Epoch 2, Index 4500, GenLoss 7.58953857421875, DiscLoss 0.0006802566349506378\n",
      "\n",
      "Epoch 2, Index 5000, GenLoss 11.733477592468262, DiscLoss 1.5948106010910124e-05\n",
      "\n",
      "Epoch 2, Index 5500, GenLoss 6.390108108520508, DiscLoss 0.037825290113687515\n",
      "\n",
      "Epoch 2, Index 6000, GenLoss 5.5314226150512695, DiscLoss 0.0054739853367209435\n",
      "\n",
      "Epoch 3, Index 0, GenLoss 38.42205810546875, DiscLoss 0.23266984522342682\n",
      "\n",
      "Epoch 3, Index 500, GenLoss 6.617011070251465, DiscLoss 0.0037229922600090504\n",
      "\n",
      "Epoch 3, Index 1000, GenLoss 11.005887985229492, DiscLoss 0.00017307247617281973\n",
      "\n",
      "Epoch 3, Index 1500, GenLoss 7.110772132873535, DiscLoss 0.0024544044863432646\n",
      "\n",
      "Epoch 3, Index 2000, GenLoss 13.352540016174316, DiscLoss 3.97489384340588e-06\n",
      "\n",
      "Epoch 3, Index 2500, GenLoss 10.78891658782959, DiscLoss 2.810212754411623e-05\n",
      "\n",
      "Epoch 3, Index 3000, GenLoss 8.888467788696289, DiscLoss 0.00018103799084201455\n",
      "\n",
      "Epoch 3, Index 3500, GenLoss 4.212686538696289, DiscLoss 0.02250773087143898\n",
      "\n",
      "Epoch 3, Index 4000, GenLoss 6.167269706726074, DiscLoss 0.001929969061166048\n",
      "\n",
      "Epoch 3, Index 4500, GenLoss 5.74680233001709, DiscLoss 0.015069800429046154\n",
      "\n",
      "Epoch 3, Index 5000, GenLoss 5.98052978515625, DiscLoss 0.0008069181349128485\n",
      "\n",
      "Epoch 3, Index 5500, GenLoss 33.814781188964844, DiscLoss 0.0\n",
      "\n",
      "Epoch 3, Index 6000, GenLoss 100.0, DiscLoss 1.1807968616485596\n",
      "\n",
      "Epoch 4, Index 0, GenLoss 55.82428741455078, DiscLoss 0.48341622948646545\n",
      "\n",
      "Epoch 4, Index 500, GenLoss 34.26134490966797, DiscLoss 0.0\n",
      "\n",
      "Epoch 4, Index 1000, GenLoss 72.5942153930664, DiscLoss 0.0\n",
      "\n",
      "Epoch 4, Index 1500, GenLoss 55.742591857910156, DiscLoss 0.0\n",
      "\n",
      "Epoch 4, Index 2000, GenLoss 56.3366813659668, DiscLoss 0.0\n",
      "\n",
      "Epoch 4, Index 2500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 4, Index 3000, GenLoss 77.24272155761719, DiscLoss 0.0\n",
      "\n",
      "Epoch 4, Index 3500, GenLoss 77.15086364746094, DiscLoss 0.0\n",
      "\n",
      "Epoch 4, Index 4000, GenLoss 77.00636291503906, DiscLoss 0.0\n",
      "\n",
      "Epoch 4, Index 4500, GenLoss 79.43981170654297, DiscLoss 0.0\n",
      "\n",
      "Epoch 4, Index 5000, GenLoss 77.17426300048828, DiscLoss 0.0\n",
      "\n",
      "Epoch 4, Index 5500, GenLoss 76.92927551269531, DiscLoss 0.0\n",
      "\n",
      "Epoch 4, Index 6000, GenLoss 77.02234649658203, DiscLoss 0.0\n",
      "\n",
      "Epoch 5, Index 0, GenLoss 78.54275512695312, DiscLoss 0.0\n",
      "\n",
      "Epoch 5, Index 500, GenLoss 76.639892578125, DiscLoss 0.0\n",
      "\n",
      "Epoch 5, Index 1000, GenLoss 80.14091491699219, DiscLoss 0.0\n",
      "\n",
      "Epoch 5, Index 1500, GenLoss 91.08067321777344, DiscLoss 0.0\n",
      "\n",
      "Epoch 5, Index 2000, GenLoss 92.19640350341797, DiscLoss 0.0\n",
      "\n",
      "Epoch 5, Index 2500, GenLoss 93.55316162109375, DiscLoss 0.0\n",
      "\n",
      "Epoch 5, Index 3000, GenLoss 94.94099426269531, DiscLoss 0.0\n",
      "\n",
      "Epoch 5, Index 3500, GenLoss 94.52283477783203, DiscLoss 0.0\n",
      "\n",
      "Epoch 5, Index 4000, GenLoss 95.72036743164062, DiscLoss 0.0\n",
      "\n",
      "Epoch 5, Index 4500, GenLoss 91.5589599609375, DiscLoss 0.0\n",
      "\n",
      "Epoch 5, Index 5000, GenLoss 92.839111328125, DiscLoss 0.0\n",
      "\n",
      "Epoch 5, Index 5500, GenLoss 91.12742614746094, DiscLoss 0.0\n",
      "\n",
      "Epoch 5, Index 6000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 6, Index 0, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 6, Index 500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 6, Index 1000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 6, Index 1500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 6, Index 2000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 6, Index 2500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 6, Index 3000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 6, Index 3500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 6, Index 4000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 6, Index 4500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 6, Index 5000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 6, Index 5500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 6, Index 6000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 7, Index 0, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 7, Index 500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 7, Index 1000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 7, Index 1500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 7, Index 2000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 7, Index 2500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 7, Index 3000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 7, Index 3500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 7, Index 4000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 7, Index 4500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 7, Index 5000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 7, Index 5500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 7, Index 6000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 8, Index 0, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 8, Index 500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 8, Index 1000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 8, Index 1500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 8, Index 2000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 8, Index 2500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 8, Index 3000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 8, Index 3500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 8, Index 4000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 8, Index 4500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 8, Index 5000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 8, Index 5500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 8, Index 6000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 9, Index 0, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 9, Index 500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 9, Index 1000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 9, Index 1500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 9, Index 2000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 9, Index 2500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 9, Index 3000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 9, Index 3500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 9, Index 4000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 9, Index 4500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 9, Index 5000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 9, Index 5500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 9, Index 6000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 10, Index 0, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 10, Index 500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 10, Index 1000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 10, Index 1500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 10, Index 2000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 10, Index 2500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 10, Index 3000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 10, Index 3500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 10, Index 4000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 10, Index 4500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 10, Index 5000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 10, Index 5500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 10, Index 6000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 11, Index 0, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 11, Index 500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 11, Index 1000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 11, Index 1500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 11, Index 2000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 11, Index 2500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 11, Index 3000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 11, Index 3500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 11, Index 4000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 11, Index 4500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 11, Index 5000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 11, Index 5500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 11, Index 6000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 12, Index 0, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 12, Index 500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 12, Index 1000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 12, Index 1500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 12, Index 2000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 12, Index 2500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 12, Index 3000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 12, Index 3500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 12, Index 4000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 12, Index 4500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 12, Index 5000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 12, Index 5500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 12, Index 6000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 13, Index 0, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 13, Index 500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 13, Index 1000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 13, Index 1500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 13, Index 2000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 13, Index 2500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 13, Index 3000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 13, Index 3500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 13, Index 4000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 13, Index 4500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 13, Index 5000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 13, Index 5500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 13, Index 6000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 14, Index 0, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 14, Index 500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 14, Index 1000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 14, Index 1500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 14, Index 2000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 14, Index 2500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 14, Index 3000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 14, Index 3500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 14, Index 4000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 14, Index 4500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 14, Index 5000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 14, Index 5500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 14, Index 6000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 15, Index 0, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 15, Index 500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 15, Index 1000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 15, Index 1500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 15, Index 2000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 15, Index 2500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 15, Index 3000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 15, Index 3500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 15, Index 4000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 15, Index 4500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 15, Index 5000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 15, Index 5500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 15, Index 6000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 16, Index 0, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 16, Index 500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 16, Index 1000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 16, Index 1500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 16, Index 2000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 16, Index 2500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 16, Index 3000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 16, Index 3500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 16, Index 4000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 16, Index 4500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 16, Index 5000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 16, Index 5500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 16, Index 6000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 17, Index 0, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 17, Index 500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 17, Index 1000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 17, Index 1500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 17, Index 2000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 17, Index 2500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 17, Index 3000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 17, Index 3500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 17, Index 4000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 17, Index 4500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 17, Index 5000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 17, Index 5500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 17, Index 6000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 18, Index 0, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 18, Index 500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 18, Index 1000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 18, Index 1500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 18, Index 2000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 18, Index 2500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 18, Index 3000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 18, Index 3500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 18, Index 4000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 18, Index 4500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 18, Index 5000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 18, Index 5500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 18, Index 6000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 19, Index 0, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 19, Index 500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 19, Index 1000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 19, Index 1500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 19, Index 2000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 19, Index 2500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 19, Index 3000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 19, Index 3500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 19, Index 4000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 19, Index 4500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 19, Index 5000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 19, Index 5500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 19, Index 6000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 20, Index 0, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 20, Index 500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 20, Index 1000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 20, Index 1500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 20, Index 2000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 20, Index 2500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 20, Index 3000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 20, Index 3500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 20, Index 4000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 20, Index 4500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 20, Index 5000, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 20, Index 5500, GenLoss 100.0, DiscLoss 0.0\n",
      "\n",
      "Epoch 20, Index 6000, GenLoss 100.0, DiscLoss 0.0\n"
     ]
    }
   ],
   "source": [
    "total_steps = len(data_loader)\n",
    "g_loss = []\n",
    "d_loss = []\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    for i,data in enumerate(data_loader):\n",
    "            optimizerd.zero_grad()\n",
    "            optimizerg.zero_grad()\n",
    "        \n",
    "        #we reshape the input data to (batch_size，-1) and send to the proper device\n",
    "            data = torch.reshape(data[0],(batch_size, -1))\n",
    "            data = data.to(comp_device)\n",
    "\n",
    "        \n",
    "        #then we create the real and fake labels which are later used as input for the \n",
    "        #BCE loss using torch.ones(batch_size, 1).to(device) and torch.zeros(batch_size, 1).to(device)\n",
    "            real_l = torch.ones(batch_size,).to(comp_device)\n",
    "            fake_l = torch.zeros(batch_size,).to(comp_device)\n",
    "        \n",
    "        #then we train the descriminator\n",
    "     #A. feedforward and store the predictions of discriminator\n",
    "            outputr = networkd(data).view(-1)\n",
    "            errD_real = criterion(outputr, real_l)\n",
    "            errD_real.backward()\n",
    "        \n",
    "        #random init a latent code z\n",
    "            lv = torch.randn(batch_size,latent_size).to(comp_device)\n",
    "        \n",
    "        #feedforward and store the predictions of generator\n",
    "            out_gF = networkg(lv)\n",
    "        \n",
    "        #feed the predictions to the descriminator and sotre the prediction\n",
    "            outputf = networkd(out_gF).view(-1)\n",
    "            errD_fake = criterion(outputf, fake_l)\n",
    "            errD_fake.backward()\n",
    "        \n",
    "        #compute BCELoss using fake images and store the loss\n",
    "            errD = errD_real + errD_fake\n",
    "        \n",
    "        #perofrm the backprop using losses after that reset the gradient of optimzier.\n",
    "            optimizerd.step()\n",
    "        \n",
    "            optimizerd.zero_grad()\n",
    "            optimizerg.zero_grad()\n",
    "        \n",
    "        #then we train the generator\n",
    "     #A. using torch.randn(batch_size, latent_size).to(device) to init a z\n",
    "    \n",
    "            lv = torch.randn(batch_size,latent_size).to(comp_device)   \n",
    "        \n",
    "            #B. feedforward and store the predictions of generator\n",
    "            out_gF = networkg(lv)\n",
    "        \n",
    "            #C. feed the predictions to the descriminator and sotre the prediction\n",
    "            outputf = networkd(out_gF).view(-1)\n",
    "        \n",
    "            #D. compute BCE_Loss using real images and store the loss\n",
    "            errG_real = criterion(outputf, real_l)\n",
    "            errG_real.backward()\n",
    "        \n",
    "            #F. perofrm the backprop using losses after that reset the gradient of optimzier.\n",
    "            optimizerg.step()\n",
    "        \n",
    "            optimizerd.zero_grad()\n",
    "            optimizerg.zero_grad()\n",
    "        \n",
    "            # Save Losses for plotting later\n",
    "            g_loss.append(errG_real.item())\n",
    "            d_loss.append(errD.item())\n",
    "            if (i%500 == 0):\n",
    "                print('\\nEpoch {}, Index {}, GenLoss {}, DiscLoss {}'.format(epoch+1,i,errG_real.item(),errD.item()))\n",
    "      ##b. we store all real image with shape (images.size(0), 1, 28, 28) inoder for the further comparision only once\n",
    "    if (epoch == 0):\n",
    "        real_images = data.reshape(data.size(0), 3, 64, 64)\n",
    "        save_image(denorm(real_images), os.path.join(filename, 'real.png'))\n",
    "    \n",
    "    ## c. we save the fake image with image shape (images.size(0), 1, 28, 28) and denorm() function\n",
    "    fake_images = out_gF.reshape(out_gF.size(0), 3, 64, 64)\n",
    "    save_image(denorm(fake_images), os.path.join(filename, 'fake.png'))\n",
    "    \n",
    "torch.save(networkg.state_dict(), filename+\"/generator\")\n",
    "torch.save(networkd.state_dict(), filename+\"/discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmvc/.conda/envs/cv19F/lib/python3.6/site-packages/IPython/core/pylabtools.py:128: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(g_loss,label=\"G\")\n",
    "plt.plot(d_loss,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
